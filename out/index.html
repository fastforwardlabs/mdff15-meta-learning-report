<!DOCTYPE html>
    <html lang="en">
      <head>
<meta charset="utf-8" />

<title>Causality for Machine Learning</title>
<meta name="description" content="An online research report on causality for machine learning by Cloudera Fast Forward." />

<meta property="og:title" content="Causality for Machine Learning" /> 
<meta property="og:description" content="An online research report on causality for machine learning by Cloudera Fast Forward." />
<meta property="og:image" content="https://ff13.fastforwardlabs.com/causality.png" />
<meta property="og:url" content="https://ff13.fastforwardlabs.com" />
<meta name="twitter:card" content="summary_large_image" />

<meta name="viewport" content="width=device-width" />
<link rel="icon" type="image/x-icon" href="favicon.ico" />

<style type="text/css">
    
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Mono';
    src: url('fonts/IBMPlexMono-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexMono-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Regular.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Regular.woff') format('woff');
    font-weight: normal;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Italic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Italic.woff') format('woff');
    font-weight: normal;
    font-style: italic;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-Bold.woff2') format('woff2'),
      url('fonts/IBMPlexSans-Bold.woff') format('woff');
    font-weight: bold;
    font-style: normal;
  }
  @font-face {
    font-family: 'Plex Sans';
    src: url('fonts/IBMPlexSans-BoldItalic.woff2') format('woff2'),
      url('fonts/IBMPlexSans-BoldItalic.woff') format('woff');
    font-weight: bold;
    font-style: italic;
  }
  
    * {
      box-sizing: border-box;
    }
    html {
      background: #fff;
      font-family: "Plex Sans", serif, sans-serif;
      font-size: 17.5px;
      line-height: 28px;
    }
    body {
      margin: 0;
    }
    .content {
      max-width: 64ch;
      padding-left: 2ch;
      padding-right: 2ch;
      margin: 0 auto;
      display: block;
      padding-bottom: 0px;
    }
   p, ul, ol {
      margin: 0;
    }
    ul, ol {
      padding-left: 3ch;
    }
  p {
   // text-indent: 3ch;
}
    li p:first-child {
      text-indent: 0;
    }

    #pdf-logo {
      display: none;
    }

   hr {
      margin: 0;
      border-top-color: black;
      margin-top: -0.5px;
      margin-bottom: 27.5px;
    }
  
h1, h2, h3, h4, h5, h6, button { font-size: inherit; line-height: inherit; font-style: inherit; font-weight: inherit; margin: 0; font-feature-settings: "tnum"; border: none; background: transparent; padding: 0;  }
button:focus, button:hover {
  background: rgba(0,0,0,0.125);
  outline: none;
}
h1 {
  font-size: 42px;
  line-height: 56px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h2 {
  font-size: 31.5px;
  line-height: 42px;
  font-weight: bold;
  margin-top: 28px;
  margin-bottom: 14px;
}
h3 {
  font-size: 26.25px;
  line-height: 35px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h4 {
  font-size: 21px;
  line-height: 28px;
  font-weight: bold;
  margin-top: 14px;
  margin-bottom: 14px;
}
h5 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-weight: bold;
}
h6 {
  font-size: 17.5px;
  line-height: 28px;
  margin-top: 14px;
  margin-bottom: 14px;
  font-style: italic;
}
p {
  margin-bottom: 14px;
}
.content {
  position: relative;
  }
figure {
  margin: 0;
  margin-top: 14px;
  margin-bottom: 28px;
  display: block;
  position: relative;
  page-break-inside: avoid;
}
blockquote {
  margin: 0;
   margin-top: 14px;
  margin-bottom: 14px;
margin-left: 2ch;
}
blockquote + blockquote {
  margin-top: 0;
}
figcaption {
  font-family: "Plex Mono", serif, monospace;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 21px;
}
.info {
  background: #efefef;
  padding-left: 2ch;
  padding-right: 2ch;
  padding-top: 14px;
  padding-bottom: 14px;
  margin-bottom: 28px;
}
.info p:last-child {
  margin-bottom: 0;
}
img {
  display: block;
  position: relative;
  max-width: 100%;
  margin: 0 auto;
  page-break-inside: avoid;
}
code {
  font-size: 0.9em;
  line-height: 1.2;
  background: rgba(0,0,0,0.125);
  padding: 0 0.3em;
}
pre {
  font-size: 0.9em;
  line-height: 1.2;
  background: rgba(0,0,0,0.125);
  overflow-x: scroll;
  max-width: 100%;
  padding-left: 1ch;
  padding-right: 1ch;
  padding-top:0.625em;
  padding-bottom:0.625em;
}
pre code {
  background: transparent;
}

table {
  min-width: 100%;
  text-align: left;
  margin-top: 14px;
  font-size: 13.125px;
  line-height: 18.900000000000002px;
  border-collapse: collapse;
}
table, th, td {
  border: solid 1px black;
}
td {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  valign: top;
  vertical-align: top;
}
th {
  padding-left: 0.5ch;
  padding-right: 0.5ch;
  vertical-align: top;
  background: #efefef;
}
table ul, table ol {
  list-style-position: inside;
  padding-left: 0;
}

  a {
    color: inherit;
  }
  .table-of-contents {
    background: #efefef;
    position: fixed;
    left: 0;
    top: 0;
    width: 32ch;
    height: 100vh;
    overflow-y: auto;
    background: #efefef;
      // background: rgba(230,230,230,0.85);
      //   backdrop-filter: blur(5px);
  }
  body {
    padding-left: 32ch;
  }
  p:empty {
    display: none;
  }
  ul, ol {
  margin-bottom: 14px;
  }

  #report-iso {
    display: none;
  }

.table-of-contents {
    counter-reset: chapters;
}
 .table-of-contents ul {
    list-style: none;
    padding-left: 0;
    margin-bottom: 0;
    padding-bottom: 0;
  }
 .table-of-contents > ul {
  }
  .table-of-contents > ul > li > a:before {
          counter-increment: chapters;
          content: counter(chapters) ". ";
          display: none;
  }
 .table-of-contents > ul > li {
    font-weight: bold;
  }

 .table-of-contents > ul > li > ul > li {
    font-weight: normal;
    font-style: normal;
    text-transform: none;
    letter-spacing: 0;
    margin-left: 0;
  }
 .table-of-contents > ul > li > ul > li > ul > li {
    font-weight: normal;
    font-style: italic;
  }
 .table-of-contents a {
    text-decoration: none;
  }
  .table-of-contents a:hover {
    text-decoration: underline;
  }
 sup {
  }
  .table-of-contents ul a {
    display: block;
    padding-left: 32px;
    text-indent: -16px;
    padding-right: 16px;
  }
  .table-of-contents ul li a.active {
    position: relative;
    background: #ddd;
  }

 .table-of-contents > ul > li > ul > li > a {
    font-size: 15.75px;
      line-height: 25.2px;
  }
  .table-of-contents > ul > li > ul > li > ul > li > a {
    padding-left: 48px;
  }

h1 {
    counter-reset: chp;
}
h2 {
  position: relative;
  display: block;
  page-break-before: always;
  padding-top: 42px;
}
  h2:before {
    position: absolute;
    left: 0;
    top: 0;
      font-size: 17.5px;
    color: black;
    counter-increment: chp;
    content: "chapter " counter(chp);
    text-transform: uppercase;
    display: none
  }

  .toc-desktop-hidden .table-of-contents {
    width: auto;
  }
  .toc-desktop-hidden #contents-label {
    display: none;
  }
  .toc-desktop-hidden .table-of-contents ul {
    display: none;
  }
  body.toc-desktop-hidden {
    padding-left: 5ch;
  }
  body:before {
    content: " ";
    height: 28px;
    width: 96ch;
    background: black;
    position: absolute;
    left: 0;
    top: 0;
    z-index: 999;
    display: none;
  }
    #toc-header {
      margin-top: 14px;
      margin-bottom: 14px;
      margin-left: 1ch;
      margin-right: 1ch;
    }

  @media screen and (max-width: 1028px) {
    h1 {
      font-size: 36.75px;
      line-height: 49px;
      font-weight: bold;
      margin-top: 14px;
      margin-bottom: 14px;
    }
    .table-of-contents ul li {
      padding-top: 3.5px;
      padding-bottom: 3.5px;
    }

    #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
    }

    body {
      padding-left: 0;
      padding-top: 42px;
    }
    .content {
        overflow-wrap: break-word;
        word-wrap: break-word;
    }
    #contents-label {
      display: none;
    }
    .table-of-contents {
      height: auto;
      width: 100%;
      z-index: 3;
    }
  body.toc-mobile-show .content:before {
      content: "";
      position: fixed;
      left: 0;
      top: 0;
      bottom: 0;
      right: 0;
      background: rgba(0,0,0,0.25);
      z-index: 2;
      border-top: solid 42px #aaa;
    }

    .table-of-contents > ul {
      display: none;
    }
   body.toc-mobile-show {
      overflow: hidden;
    }
    body.toc-mobile-show #toc-header {
      margin-top: 7px;
      margin-bottom: 7px;
      position: relative;
    }
    body.toc-mobile-show .table-of-contents {
      width: 32ch;
      height: 100vh;
      max-width: calc(100% - 4ch);
      overflow: auto;
    }
   body.toc-mobile-show .table-of-contents > ul {
      display: block;
      padding-bottom: 28px;
      position: relative;
    }
    body.toc-mobile-show #contents-label {
      display: inline;
      position: relative;
    }
  }
}
</style>
<script>
    function inViewport(elem) {
      let bounding = elem.getBoundingClientRect();
      return (
        bounding.top >= 0 &&
        bounding.left >= 0 &&
        bounding.bottom <= (window.innerHeight || document.documentElement.clientHeight) &&
        bounding.right <= (window.innerWidth || document.documentElement.clientWidth)
      );
    };

    function setActive(target_id) {
      let selector = '.table-of-contents ul li a[href="#' + target_id + '"]'
      let link = document.querySelector(selector)
      if (link !== null) {
        link.className = 'active'
      }
    }

    window.addEventListener("load", (event) => {
      let headings = document.querySelectorAll('h2, h3');
      let links = document.querySelectorAll('.table-of-contents ul li a')

      observer = new IntersectionObserver((entry, observer) => {
        if (entry[0].intersectionRatio === 1) {
          for (let link of links) {
            link.className = ''
          }
          let target_id = entry[0].target.getAttribute('id')
          setActive(target_id)
        }
      }, { threshold: 1, rootMargin: "0px 0px -50% 0px" });

      let first = true
      for (let heading of headings) {
        if (first && inViewport(heading)) {
          setActive(heading.getAttribute('id'))
          first = false
        }
        observer.observe(heading);
      }

      document.querySelector('#toggle_contents').addEventListener('click', () => {
        let body = document.body
        if (window.innerWidth > 1027) {
          let hidden_class = "toc-desktop-hidden"
          if (body.className === hidden_class) {
            body.className = ''
          } else {
            body.className = hidden_class
          }
        } else {
          let show_class = "toc-mobile-show"
          if (body.className === show_class) {
            body.className = ''
          } else {
            body.className = show_class
          }
        }
      })

      for (let link of links) {
        link.addEventListener('click', (e) => {
          let href = e.target.getAttribute('href')
          let elem = document.querySelector(href)
          window.scroll({
            top: elem.offsetTop - 28,
            left: 0,
            behavior: 'smooth'
          })
          if (window.innerWidth < 1028) {
            document.body.className = ''
          }
          e.preventDefault() 
        })
      }

      document.querySelector('.content').addEventListener('click', () => {
        if (window.innerWidth < 1028) {
          document.body.className = ''
        }
      })
      document.querySelector('.table-of-contents').addEventListener('click', (e) => {
        e.stopPropagation()
      })

      let mediaQueryList = window.matchMedia("(max-width: 1028px)");
      function handleBreakpoint(mql) {
        // clear any left over toggle classes
        document.body.className = ''
      }
      mediaQueryList.addListener(handleBreakpoint);
    }, false);
  </script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-157475426-6', 'auto');
  ga('send', 'pageview');

  window.addEventListener('load', function() {
    document.getElementById('report-pdf-download').addEventListener('click', function() {
      ga('send', {
        hitType: 'pageview',
        page: '//FF13-Causality_for_Machine_Learning-Cloudera_Fast_Forward.pdf'
      });
    });
  })

</script>
<!-- End Google Analytics -->
</head>
      <body>
        <div class="content" style="position: relative;">
          <div id="html-logo" style="margin-top: 28px; line-height: 0; display: flex;">
            <a href="https://www.cloudera.com/products/fast-forward-labs-research.html"><img alt="Cloudera Fast Forward" style="display: block; height: 14px; margin-bottom: 7px;" src='/figures/cloudera-fast-forward-logo.png' /></a>
          </div>
          <div id="pdf-logo" style="margin-top: 28px; ">
            <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Cloudera Fast Forward</a>
          </div>
          <h1 id="meta-learning">Meta-Learning</h1>
<p>FF15 · <em>September 2020</em></p>
<figure><img src="figures/ff15-cover-splash.png" alt=""></figure>
<p><em>This is an applied research report by <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Cloudera Fast Forward</a>. We write reports about emerging technologies. Read our full report on meta-learning below or <a href="/FF15-Meta-Learning-Cloudera_Fast_Forward.pdf" target="_blank" id="report-pdf-download">download the PDF</a>.</em></p>
<p><div class="table-of-contents"><div id="toc-header" style="display: flex; font-weight: bold; text-transform: uppercase;">
     <div><button id="toggle_contents" style="padding-left: 0.5ch; padding-right: 0.5ch; cursor: pointer; position: relative; top: -1px;">☰</button><span id="contents-label" style="margin-left: 0;"> Contents</span></div>
  </div><ul><li><a href="#introduction">Introduction</a><ul><li><a href="#why-should-we-care%3F">Why should we care?</a></li><li><a href="#why-now%3F">Why now?</a></li></ul></li><li><a href="#framing-the-problem">Framing the problem</a></li><li><a href="#solving-the-problem">Solving the problem</a><ul><li><a href="#data-set-up">Data set-up</a></li><li><a href="#meta-learning%3A-learning-to-learn">Meta-learning: learning to learn</a></li><li><a href="#model-agnostic-meta-learning-(maml)">Model Agnostic Meta-learning (MAML)</a></li></ul></li><li><a href="#experiment">Experiment</a><ul><li><a href="#dataset">Dataset</a></li><li><a href="#set-up">Set-up</a></li><li><a href="#results">Results</a></li></ul></li><li><a href="#challenges-and-ways-to-overcome">Challenges and ways to overcome</a></li><li><a href="#ethics">Ethics</a></li><li><a href="#moving-forward">Moving forward</a><ul><li><a href="#author%E2%80%99s-note">Author’s note</a></li></ul></li></ul></div></p>
<p><em>In early spring of 2019, we researched approaches that would allow a machine learning practitioner to perform supervised learning with only a
limited number of examples available during training. This search led us to a new paradigm: meta-learning, in which an algorithm not only learns
from a handful of examples, but also learns to classify novel classes during model inference. We decided to focus our research report—<a href="https://blog.fastforwardlabs.com/2019/04/02/a-guide-to-learning-with-limited-labeled-data.html">Learning
with Limited Labeled Data</a>—on active learning for
deep neural networks, but we were both intrigued and fascinated with meta-learning as an emerging capability. This article is an attempt to throw
some light on the great work that’s been done in this area so far.</em></p>
<h2 id="introduction">Introduction</h2>
<p>Humans have an innate ability to learn new skills quickly. For example, we can look at one instance of a knife and be able to discriminate all knives from other cutlery items, like spoons and forks. Our ability to learn new skills and adapt to new environments quickly (based on only a few experiences or demonstrations) is not just limited to identifying new objects, learning a new language, or figuring out how to use a new tool;  our capabilities are much more varied. In contrast, machines—especially deep learning algorithms—typically learn quite differently. They require vast amounts of data and compute and may yet struggle to generalize. The reason humans are successful in adapting and learning quickly is that they leverage knowledge acquired from prior experience to solve novel tasks. In a similar fashion, meta-learning leverages previous knowledge acquired from data to solve novel tasks quickly and more efficiently.</p>
<figure><img src="figures/ff15-44.png" alt="Figure 1: Humans can learn things quickly"><figcaption>Figure 1: Humans can learn things quickly</figcaption></figure>
<h3 id="why-should-we-care%3F">Why should we care?</h3>
<p>An experienced ML practitioner might wonder, isn’t this covered by recent (and much-accoladed) advances in transfer learning? Well, no. Not exactly.
First, supervised learning through deep learning methods requires massive amounts of labeled training data. These datasets are expensive to create, especially when one needs to involve a domain expert. While pre-training is beneficial, these approaches become less effective for domain-specific problems, which still require large amounts of task-specific labeled data to achieve good performance.</p>
<p>In addition, certain real world problems have long-tailed and imbalanced data distributions, which may make it difficult to collect training
examples.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> For instance, in the case of
search engines, perhaps a few keywords are commonly searched for, whereas a vast majority of keywords are rarely searched for. This may result in
poor performance of models/applications based on long-tailed or imbalanced data distributions. The same could be true of recommendation engines;
when there are not enough user reviews or ratings for obscure movies or products, it can
hinder model performance.</p>
<figure><img src="figures/ff15-45.png" alt="Figure 2: Long-tailed distributions"><figcaption>Figure 2: Long-tailed distributions</figcaption></figure>
<p>Most important, the ability to learn new tasks quickly during model inference is something that conventional machine learning approaches do not attempt. This is what makes meta-learning particularly attractive.</p>
<h3 id="why-now%3F">Why now?</h3>
<p>From a deep learning perspective, meta-learning is particularly exciting and adoptable for three reasons: the ability to learn from a handful of examples, learning or adapting to novel tasks quickly, and the capability to build more generalizable systems. These are also some of the reasons why meta-learning is successful in applications that require data-efficient approaches; for example, robots are tasked with learning new skills in the real world, and are often faced with new environments.</p>
<p>Further, computer vision is one of the major areas in which meta-learning techniques have been explored to solve few-shot learning
problems—including classification, object detection and segmentation, landmark prediction, video synthesis, and others.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>(https://arxiv.org/abs/2004.05439) Additionally, meta-learning has been popular in language modeling tasks, like filling in missing words<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>(https://arxiv.org/abs/1606.04080) and machine translation<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>(https://arxiv.org/abs/1808.08437), and is also being applied to speech recognition tasks, like cross-accent adaptation.<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>(https://arxiv.org/abs/2003.01901)</p>
<figure><img src="figures/ff15-46.png" alt="Figure 3: Applications - object detection, machine translation, missing words"><figcaption>Figure 3: Applications - object detection, machine translation, missing words</figcaption></figure>
<p>As with any other machine learning capability that starts to show promise, there are now libraries and tooling that make meta-learning
more accessible. Although not entirely production-ready, libraries like <a href="https://github.com/tristandeleu/pytorch-meta">torch-meta</a>, <a href="https://github.com/learnables/learn2learn">learn2learn</a> and <a href="https://github.com/google-research/meta-dataset">meta-datasets</a> help handle data, simplify processes when used with popular deep learning frameworks, and help document and benchmark performance on datasets.</p>
<p>The rest of this report, along with its accompanying code, explores meta-learning, provides insight into how it works, and discusses its
implications. We’ll do this using a simple, yet elegant algorithm—Model Agnostic Meta-Learning<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>(https://arxiv.org/pdf/1703.03400.pdf)—applied to a few-shot classification problem, which was proposed a while ago, but
continues to provide a good basis for extension and modification even today.</p>
<h2 id="framing-the-problem">Framing the problem</h2>
<p>What kind of problems can meta-learning help us solve? One of the most popular categories is few-shot learning. In a few-shot learning scenario, we have only a limited number of examples on which to perform supervised learning, and it is important to learn effectively from them. The ability to do so could help relieve the data-gathering burden (which at times may not even be possible).</p>
<p>Let’s say we want to solve a few-shot classification problem, shown in Figure(4) below. Usually the few-shot classification problem is set up as a N-way k-shot problem, where N is the number of classes and k is the number of examples in each class. For example, let’s say we are given an image from each of five different classes (that is, N=5 and k=1) and we are supposed to classify new images as belonging to one of these classes. What can we do? How would one normally model this?</p>
<figure><img src="figures/ff15-47.png" alt="Figure 4: A few-shot classification (5-way, 1-shot) problem"><figcaption>Figure 4: A few-shot classification (5-way, 1-shot) problem</figcaption></figure>
<p>One way to solve the problem would be to train a neural network model from scratch on the five training images. At a high level, a training step
will look something like Figure(5) below. The neural network model is randomly initialized and receives an image (or images) as input.
It then predicts the output label(s) based on the initial model parameters. The difference between the true label(s) and the predicted label(s)
is measured by a loss function (for example, cross-entropy), which in turn is used to compute the gradients. The gradients are then used to help
calculate new model parameters that best reduce the difference between the “true” and predicted labels. This entire step is known as
backpropagation. After backpropagation, the optimizer updates the model parameters for the model, and all of these steps are repeated for the
rest of the images and/or for some number of epochs, until the loss, evaluated on the train or test data, falls below an acceptable level.</p>
<figure><img src="figures/ff15-48.png" alt="Figure 5: A training step in normal training process, adopted from HuggingFace’s blog post, “From zero to research”"><figcaption>Figure 5: A training step in normal training process, adopted from HuggingFace’s blog post, <a href="https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#0f06">“From zero to research”</a></figcaption></figure>
<p>With only five images available for training, chances are that we would likely overfit and perform poorly on the test images. Adding some
regularization or data augmentation may alleviate this problem to some extent, but it will not necessarily solve it. The very nature of a
few-shot problem makes it hard to solve, as there is no prior knowledge of the tasks.</p>
<p>Another possible way to solve the problem could be to use a pre-trained network from another task, and then fine-tune it on the five training
images. However, depending on the problem, this may not always be feasible, especially if the task the network was trained on differs substantially.</p>
<h2 id="solving-the-problem">Solving the problem</h2>
<h3 id="data-set-up">Data set-up</h3>
<p>What meta-learning proposes is to use an end-to-end deep learning algorithm that can learn a representation better suited for few-shot learning.
It is similar to the pre-trained network approach, except that it learns an initialization that serves as a good starting point for the handful of
training data points. In the few-shot classification problem  discussed, we could leverage training data that’s available from other image
classes, for instance, we could look at the training data available and use images from classes like mushrooms, dogs, eyewear, etc. The model
could then build up prior knowledge such that, at inference time, it can quickly acquire task-specific knowledge with only a handful of training
examples.  This way, the model first learns parameters from a training dataset that consists of images from other classes, and then uses those
parameters as prior knowledge to tune them further, based on the limited training set (in this case, the one with five training examples).</p>
<p>Now the question is, how can the model learn a good initial set of parameters which can then be easily adapted to the downstream tasks?
The answer lies in a simple training principle, which was initially proposed by Vinyals et. al.<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>:</p>
<div class="info">
<center>Train and test conditions must match</center>
</div>
<p>The idea is to train a model by showing it only a few examples per class, and then test it against examples from the same classes that have been
held out from the original dataset, much the way it will be tested when presented with only a few training examples from novel classes. Each
training example, in this case, comprises pairs of train and test data points called an <em>episode</em>.</p>
<figure><img src="figures/ff15-49.png" alt="Figure 6: Meta-learning data setup, adopted from Optimization as a Model for Few-Shot Learning (PDF)"><figcaption>Figure 6: Meta-learning data setup, adopted from <a href="https://openreview.net/pdf?id=rJY0-Kcll">Optimization as a Model for Few-Shot Learning (PDF)</a></figcaption></figure>
<p>This is a departure from the way that data is set up for conventional supervised learning. The training data (also called the meta-training data)
is composed of train and test examples, alternately referred to as the support and query set.</p>
<div class="info">
<p>The number of classes <em>(N)</em> in the support set defines a task as an <em>N</em>-class classification task or <em>N</em>-way task, and the number of labeled
examples in each class <em>(k)</em> corresponds to <em>k</em>-shot, making it an <em>N</em>-way, <em>k</em>-shot learning problem.</p>
</div>
<p>In this case, we have a 5-way, 1-shot learning problem.</p>
<p>Similar to conventional supervised learning, which sets aside validation and test datasets for hyper-parameter tuning and generalization,
meta-learning also has meta-validation and meta-test sets. These are organized in a similar fashion as the meta-training dataset in episodes,
each with support and query sets; the only difference is that the class categories are split into meta-training, validation, and test datasets,
such that the classes do not overlap.</p>
<h3 id="meta-learning%3A-learning-to-learn">Meta-learning: learning to learn</h3>
<p>A meta-learning model should be trained on a variety of tasks, and then optimized further for novel tasks. A task, in this case, is basically a
supervised learning problem (like image classification or regression). The idea is to extract prior information from a set of tasks that allows
efficient learning on new tasks. For our image classification problem, the ideal set-up would include many classes, with at least a few examples
for each. These can then be used as a meta-training set to extract prior information, such that when a new task like the one in the Figure(4)
above comes in, the model can perform it more efficiently.</p>
<p>At a high level, the meta-learning process has two phases: meta-learning and adaptation. In the meta-learning phase, the model learns an initial
set of parameters slowly across tasks; during the adaptation phase, it focuses on quick acquisition of knowledge to learn task-specific
parameters. Since the learning happens at two levels, meta-learning is also known as learning to learn.<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup></p>
<p>A variety of approaches have been proposed that vary based on how the adaptation portion of the training process performs. These can broadly be classified into three categories: “black-box” or model-based, metric-based, and optimization-based approaches.</p>
<p>“Black-box” (or model-based) approaches simply train an entire neural network, given some training examples in the support set and an initial
set of meta-parameters, and then make predictions on the query set. They approach the problem as supervised learning, although there are
approaches that try to eliminate the need to learn an entire network.<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></p>
<p>Metric-based approaches usually employ non-parametric techniques (for example, <em>k</em>-nearest neighbors) for learning. The core idea is to learn a
feature representation (e.g.,  learning an embedding network that transforms raw inputs into a representation which allows similarity comparison
between the support set and the query set). Thus, performance depends on the chosen similarity metric (like cosine similarity or euclidean
distance).</p>
<p>Finally, optimization-based approaches treat the adaptation part of the process as an optimization problem. This article mainly focuses on one of
the well-known approaches in this category, but before we delve into it, let’s look at how optimization-based learning actually works.</p>
<p>During training, we iterate over datasets of episodes. In meta-training, we start with the first episode, and the meta-learner takes the training
(support) set and produces a learner (or a model) that will take as input the test (query) set and make predictions on it. The meta-learning
objective is based on a loss (for example, cross-entropy) that is derived from the test or query set examples and will backpropagate through these
errors. The parameters of the meta-learner (that is, meta-parameters) are then updated based on these errors to optimize the loss.<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></p>
<p>In the next step, we look at the next episode, train on the support set examples, make predictions on the query set, update meta-parameters, and
repeat. In attempting to learn a meta-learner this way, we are trying to solve the problem of generalization. The examples in the test (or query)
set are not part of the training—so, in a way, the meta-learner is learning to extrapolate.</p>
<figure><img src="figures/ff15-50.png" alt="Figure 7: Learning to learn"><figcaption>Figure 7: Learning to learn</figcaption></figure>
<h3 id="model-agnostic-meta-learning-(maml)">Model Agnostic Meta-learning (MAML)</h3>
<p>Now that we have a general idea of how meta-learning works, the rest of this article mainly focuses on MAML<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>, which is perhaps one of the best known optimization-based approaches.
While there have been more recent extensions to it, MAML continues to serve as a foundational approach.</p>
<p>The goal of meta-learning is to help the model quickly adapt to or learn on a new task, based on only a few examples. In order for that to happen,
the meta-learning process has to help the model learn from a large number of tasks.  For example, for the image classification problem we’ve
considered, the new task is the one shown in Figure(4), while the large number of tasks could be images from other classes that are utilized for
building a meta-training dataset, as shown in Figure(6).</p>
<p>The key idea in MAML is to establish initial model parameters in the meta-training phase that maximize its performance on the new task. This is
done by updating the initial model parameters with a few gradient steps on the new task. Training the model parameters in this way allows the
model to learn an internal feature representation that is broadly suitable for many tasks—the intuition being that learning an initialization that
is good enough, and then fine-tuning the model slightly, will produce good results.</p>
<p>Imagine we have two neural network models that share the same model architecture:<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup> <em>learner</em> for the meta-learning process and <em>adapter</em> for
the adaptation process. Since we have two models to train, we also have two different learning rates associated with them. The MAML algorithm can
then be summarized in the following steps:</p>
<div class="info">
<ul>
<li>Step 1: Randomly initialize the learner</li>
<li>Step 2: Repeat the entire process from Step (2.a) to Step (3) for all the episodes of the meta-training dataset (or for a certain number of epochs) until the learner converges to a good set of “meta-parameters.”
<ul>
<li>Step 2.a: Sample a batch of episodes from the meta-training dataset</li>
<li>Step 2.b: Initialize the adapter with the learner’s parameters</li>
<li>Step 2.c: While number of inner training steps is not equal to zero
<ul>
<li>Step 2.c.1: Train the adapter based on the support set(s) of the batch, compute the loss and the gradients, and update the adapter’s parameters</li>
</ul>
</li>
<li>Step 2.d: Use the updated parameters of the adapter to compute the “meta-loss” based on the query set(s) of the batch</li>
</ul>
</li>
<li>Step 3: Compute the “meta-gradients”, followed by the “meta-parameters” based on the “meta-loss,” and update the learner’s parameters</li>
</ul>
</div>
<p>The “meta-loss” indicates how well the model is performing on the task. In effect, the <em>learner</em> is being fine-tuned using a gradient-based
approach for every new task in the batch of episodes. Further, the <em>learner</em> acts as initialization parameters for the <em>adapter</em> so that it can
perform task-specific learning.</p>
<figure><img src="figures/ff15-51.png" alt="Figure 8: MAML"><figcaption>Figure 8: MAML</figcaption></figure>
<p>During inference, we actually use the meta-trained model (<em>learner</em>) to predict on the meta-test set, except this time—although the meta-trained
model undergoes additional gradient steps to help classify the query set examples—the <em>learner</em> parameters aren’t updated.</p>
<p>As long as the model is trained using gradient descent, the approach does not place any constraints on the model architecture or the loss
function. This characteristic makes it applicable to a wide variety of problems, including regression, classification, and reinforcement learning.
Further, since the approach actually undergoes a few gradient steps for a novel task, it allows the model to perform better on out-of-sample data,
and hence achieves better generalization. This behavior can be attributed to the central assumption of meta-learning: that the tasks are
inherently related and thus data-driven inductive bias can be leveraged to achieve better generalization.</p>
<h2 id="experiment">Experiment</h2>
<p><a href="https://arxiv.org/pdf/1703.03400.pdf">The MAML paper</a> explores the approach for multiple problems: regression, classification, and reinforcement
learning. To gain a better understanding of the algorithm and investigate whether MAML really learns to adapt to novel tasks, we tested the
technique on the <a href="https://quickdraw.withgoogle.com/data">Quick, Draw!</a> dataset. All of the experiments were performed using PyTorch (which allows
for automatic differentiation of the gradient updates), along with the <a href="https://github.com/tristandeleu/pytorch-meta">torch-meta</a> library. The
torch-meta library provides data loaders for few-shot learning, and extends PyTorch’s Module class to simplify the inclusion of additional
parameters for different modules for meta-learning. This functionality allows one to backpropagate through an update of parameters, which is a key
ingredient for gradient-based meta-learning. While torch-meta provides an excellent structure for creating reproducible benchmarks, it will be
interesting to see its integration with other meta-learning approaches that handle datasets differently, and its flexibility in adopting them in
the future. For our purposes, we extended the torch-meta code to accommodate the Quick, Draw! data set-up. The experiment code is available
<a href="">here</a>.</p>
<h3 id="dataset">Dataset</h3>
<p>The Quick, Draw! dataset consists of 50 million doodles (hand-drawn figures) across 345 categories. We conducted two experiments: in one, we
randomly selected 100 images; in the other, we randomly selected 20 images per class. The 345 classes were randomly split into
meta-train/validation/test datasets as 207/69/69. The training and evaluation was performed on the meta-training set. The meta-validation set was
mostly used for hyper-parameter tuning, and the meta-test set measured the generalization to new tasks.</p>
<h3 id="set-up">Set-up</h3>
<p>We evaluated the MAML approach on 5-way 1/5/10-shot and 10-way 1/5/10-shot settings for the Quick, Draw! dataset. An experiment on each of the
100-sample and 20-sample datasets consisted of training for 50 epochs with each epoch consisting of 100 batches of tasks, where a task’s batch
size was 25 for 100-sample and 10 for 20-sample datasets. At the end of an epoch, we evaluated the model performance on the meta-validation dataset. At the end of 50 epochs, we evaluated the
model on the meta-test dataset.</p>
<figure><img src="figures/9.png" alt="Figure 9: 5-way, 1-shot episode example"><figcaption>Figure 9: 5-way, 1-shot episode example</figcaption></figure>
<p>&lt;&lt;to-do: fix model architecture&gt;&gt;</p>
<p>In terms of model architecture, we used a network with 4 convolution layers—with size 20 channels in the intermediate representations, each
including batch normalization and ReLU nonlinearities, followed by a linear layer. For all models, the loss function was the cross-entropy error between the predicted and true labels.</p>
<p>The models for the 100-sample dataset were trained using an SGD optimizer with a learning rate of 0.001, an inner learning rate of 0.01 for the
adaptation process, a step size (that is, number of gradient steps) of 5, and a task batch size of 25. All the hyper-parameters were the same for
all the models, for a consistent comparison. While the models for the 20-sample dataset were trained with a slightly lower learning rate of 0.0005, an inner learning rate of 0.005,  a task batch size of 10 along with the rest of the parameters were same as the 100-sample dataset.</p>
<h3 id="results">Results</h3>
<p>&lt;&lt;to-do: update result images, and also update 20 sample runs and corresponding sentences&gt;&gt;</p>
<p>The figures below illustrate how MAML performs on the 100- and 20-item randomly sampled versions of the Quick, Draw! dataset, for a 5-way or a
10-way classification few-shot problem, with a varying number of examples per class. As expected, the model performance on the both the
meta-validation and meta-test set is better when the model is trained on a 100-sample subset instead of using just 20 samples. Further,
5-way classification yields better results
than 10-way classification— which is to be expected, given that 5-way classification is an easier task than 10-way classification. Also, as the
number of shots/examples per class increase, we see better performance during validation and test time. The validation results for 5-way
1/5/10-shot learning based on 20 samples look promising too. In the 10-way learning based on 20-samples we see some overfitting after a few epochs and may want to restrain the model by stopping early. That said, we have left them as is for easy comparison with the rest of the experiment results.</p>
<figure><img src="figures/10.png" alt="Figure 10. 5-way, 1/5/10-shot results based on 100 random sampled images"><figcaption>Figure 10. 5-way, 1/5/10-shot results based on 100 random sampled images</figcaption></figure>
<figure><img src="figures/11.png" alt="Figure 11. 10-way, 1/5/10-shot results based on 100 random sampled images"><figcaption>Figure 11. 10-way, 1/5/10-shot results based on 100 random sampled images</figcaption></figure>
<figure><img src="figures/12.png" alt="Figure 12. 5-way, 1/5/10-shot results based on 20 random sampled images"><figcaption>Figure 12. 5-way, 1/5/10-shot results based on 20 random sampled images</figcaption></figure>
<figure><img src="figures/13.png" alt="Figure 13. 10-way, 1/5/10-shot results based on 20 random sampled images"><figcaption>Figure 13. 10-way, 1/5/10-shot results based on 20 random sampled images</figcaption></figure>
<figure><img src="figures/14.png" alt="Figure 14. Meta-test dataset results"><figcaption>Figure 14. Meta-test dataset results</figcaption></figure>
<p>Our results demonstrate that the MAML approach is beneficial for learning with only a few examples. In the 100 randomly sampled images scenario,
the 5-way classification task gives an accuracy of around 68% with just one example. The model performance is
even better with additional examples; for both 5 examples and 10 examples per class, accuracy shoots over 80%. As expected, for the 10-way
classification task, the results are lower (by around 10-15%) but still promising.</p>
<p>For the 20-random sample scenario and a more realistic one from a meta-learning point of view, the 5-way results are still pretty good ~60% accuracy with just one example. The 10-way classification results are lower similar to the 100-sample dataset. Nonetheless, overall the results are promising even with minimal tuning and suggests the applicability of the approach for fast adaptive learning.</p>
<h2 id="challenges-and-ways-to-overcome">Challenges and ways to overcome</h2>
<p>The MAML approach fine-tunes its model using gradient descent each time for a new task. This requires it to backpropagate the meta-loss through
the model’s gradients, which involves computing derivatives of derivatives, i.e., second derivatives. While the gradient descent at test time helps it extrapolate better, it does have its costs.</p>
<p>Backpropagating through many inner steps can be compute and memory intensive. With only a few gradient steps, it might be a less time-consuming
endeavor, but it may not be the best solution for scenarios that require a higher number of gradient steps at test time. That said, the authors of
the <a href="https://arxiv.org/pdf/1703.03400.pdf">MAML paper</a> also propose a first-order approximation that eliminates the need to compute the second
derivatives, with a comparable performance. Another closely related work is OpenAI’s Reptile;<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup> it builds on first-order MAML, but doesn’t need to split the episode into support and query sets, making it a
natural choice in certain settings. However, experiments suggest that approaches to reduce computation time while not sacrificing generalization
performance are still in the works.<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup></p>
<p>As we saw previously, learning occurs in two stages: gradual learning is performed across tasks, and rapid learning is performed within tasks.
This requires two learning rates, which introduces difficulty in choosing hyper-parameters that would help achieve training stability.
The two learning rates introduce hyper-parameter grid search computation, and hence, time and resources. It is also important to select the learning rate for the adaptation process carefully because it is learning over only a few examples. In that regard, some solutions or extensions to MAML
have been developed to reduce the need for grid search or hyper-parameter tuning. For example, Alpha MAML<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> eliminates the need to tune both the learning rates by automatically updating them as needed. MAML++, on the other hand, proposes updating the query set loss (meta-loss) for every training step in the adaptation process, which can help get rid of the training instabilities. In addition, they suggest various other steps to make it computationally efficient.</p>
<p>Research that performs neural architecture search for gradient-based meta-learners<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> also suggests that approaches like MAML and its extensions tend to perform better with deeper neural architectures for few-shot classification tasks. While note-worthy, it nevertheless should be explored further with more experiments.</p>
<h2 id="ethics">Ethics</h2>
<p>Meta-learning alleviates the need to collect vast amounts of data, and hence is applicable where supervised training examples are difficult (or
even impossible) to acquire, given safety, security and privacy issues. If training efficient deep learning models is possible in such a scenario
with just a handful of examples, it will benefit machine learning practitioners and its overall adoption.</p>
<p>Recent research<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup> in fairness addresses the question of how a practitioner who has access to only a few labeled examples can successfully train a fair machine learning model. The paper suggests that one can do so by extending the MAML algorithm to Fair-MAML, such that each task includes a fairness regularization term in
the task losses and a fairness hyperparameter—gamma—in hopes of encouraging MAML to learn generalizable internal representations that strike a desirable balance between accuracy and fairness.</p>
<h2 id="moving-forward">Moving forward</h2>
<p>Meta-learning is appealing; its ability to learn from a few examples makes it particularly attractive. A gradient-based approach like MAML puts us in familiar territory: using pre-trained models and fine-tuning them. The MAML algorithm is simple, and its ability to perform a few gradient steps at inference time allows it to generalize quickly to unseen classes. The approach is applicable to a variety of problems—including regression, classification, and reinforcement learning—and can be combined with any model architecture, as long as the model is trained based on gradient descent.</p>
<p>While there are many areas of future research on meta-learning, here is our perspective on what could make it more adoptable in real world scenarios, as well as which areas could benefit from future work.</p>
<p>Some of the success of meta-learning in solving few-shot learning problems can be attributed to the way the data is set up for training and testing: episodes. In general, <em>N</em>-way, <em>k</em>-shot learning is much easier if you train the model to do <em>N</em>-way, <em>k</em>-shot learning. For our experiments,
as well as the various approaches in this area, how we defined an episode was pretty arbitrary. For instance, we made an assumption that during meta-test time we would face a 5-way or a 10-way problem, with each class having either a single example or five examples. Will a real world inference scenario always match this expectation? Likely not. It could be more valuable to find an approach that can relax this assumption.</p>
<p>Another area worth exploring is whether it is possible to train on heterogeneous datasets. For example, can we train a meta-learning model to classify a fork based on doodles,  photos of forks at restaurants, or images from product catalogs (basically, forks from different training domains or environments)? How should we define the meta-training set and episodes in such a scenario? Should we consider classes from all the environments to define the meta-train/validation/test datasets? A recent paper that applies meta-learning for few-shot land cover classification,<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>  in which the class images vary by regions (urban areas, continents, or vegetation) suggests using classes from one environment for the support set and classes from another environment for the query set. The authors find that meta-learning (MAML, actually) can benefit Earth Sciences, especially when there is a high degree of diversity in the data.</p>
<p>In real world scenarios, it’s often likely that we’ll have lots of unlabeled data. In such circumstances, is it possible to employ meta-learning algorithms to leverage these unlabeled datasets? The authors of one research paper<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> propose a solution to this by augmenting a metric-based meta-learning approach to leverage unlabeled examples. In addition to the support set and the query set, the episode includes an unlabeled set. This unlabeled set may or may not contain examples from the support set classes. The idea is to use the labeled examples from the support set and the unlabeled examples within each episode to generalize for a good performance on the corresponding query set. Experiments show an improvement in model performance in some cases.</p>
<p>Another interesting idea that’s being explored is at the crossroads of active learning<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup> and meta-learning. The field of active learning takes advantage of machine learning in collaboration with humans, selecting examples from vast pools of unlabeled data and requesting labels for them. At times, these examples are chosen based on how uncertain the model is about its predicted label, or by how “different” it is than the rest, etc.—with the ultimate aim to improve model performance. Since there are fewer labeled training examples to begin with, one could employ meta-learning approaches in such a scenario; for instance, a metric-based approach has been discussed for batch-mode active learning.<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup> (It is also possible to learn a label acquisition strategy instead.)<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup></p>
<p>Over the coming years, we will see additional approaches that will make meta-learning even more adoptable in real world scenarios; whether it’s using meta-learning successfully on heterogeneous data or leveraging unlabeled data, these approaches will make learning and generalizing with fewer labeled examples possible. As previously mentioned,  we will continue to see research that simplifies both the training and inference process. These advances will allow machine learning practitioners to develop even more new ways of designing machine learning systems.</p>
<h3 id="author%E2%80%99s-note">Author’s note</h3>
<p>Thank you so much for reading this article. This work has been deeply influenced by the work of <a href="https://ai.stanford.edu/~cbfinn/">Professor Chelsea Finn</a>. Also, the torch-meta library, along with its demo examples, made it easier to understand and showcase meta-learning.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://papers.nips.cc/paper/7278-learning-to-model-the-tail.pdf">Learning to Model the Tail (PDF)</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Meta-learning in Neural Networks: A Survey <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Matching Networks for One-Shot Learning <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Meta-Learning for Low-Resource Neural Machine Translation <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Learning Fast Adaptation on Cross-Accented Speech Recognition <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Model Agnostic Meta-learning for Fast Adaptation of Deep Networks (PDF) <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://arxiv.org/abs/1606.04080">Matching Networks for One-Shot Learning</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Thrun S., Pratt L. (eds). <a href="https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1">Learning to Learn</a>. Springer, Boston, MA. 1998. <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1">One-shot Learning with Memory-Augmented Neural Networks</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Note that this
differs from a conventional supervised learning set up, in which the objective is based on a loss derived only from the training set, and, of
course, there is no support or query set! <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p><a href="https://arxiv.org/pdf/1703.03400.pdf">Model Agnostic Meta-learning for Fast Adaptation of Deep Networks (PDF)</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>While it is possible to have two duplicate models that can
share parameter tensors in popular deep learning frameworks like PyTorch, libraries like <a href="https://github.com/tristandeleu/pytorch-meta">torch-meta</a>
have extended the existing torch modules to allow storing additional/new parameters. <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="https://arxiv.org/pdf/1803.02999.pdf">On First-Order Meta-Learning Algorithms</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p><a href="https://arxiv.org/pdf/1810.09502.pdf">How to train your MAML</a> <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p><a href="https://arxiv.org/abs/1905.07435">Alpha MAML: Adaptive Model Agnostic Meta Learning</a> <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p><a href="https://arxiv.org/abs/1806.06927">Auto-Meta: Automated Gradient Based Meta Learner Search</a> <a href="#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p><a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372839">Fairness warnings and fair-MAML: learning fairly with minimal data</a> <a href="#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p><a href="https://arxiv.org/pdf/2004.13390.pdf">Meta-learning for Few-shot Land Cover Classification</a> <a href="#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p><a href="https://arxiv.org/pdf/1803.00676.pdf">Meta-learning for semi-supervised few-shot classification</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p><a href="https://blog.cloudera.com/a-guide-to-learning-with-limited-labeled-data/">A Guide to Learning with Limited Labeled Data
</a> <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p><a href="https://openreview.net/pdf?id=r1PsGFJPz">Meta-learning for Batch Mode Active Learning (PDF)</a> <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p><a href="https://arxiv.org/abs/1806.04798">Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning</a> <a href="#fnref22" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

        </div>
      </body>
   </html>
  